Fri Oct 17 04:30:08 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:16:00.0 Off |                    0 |
| N/A   36C    P0             40W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-SXM2-32GB           On  |   00000000:3B:00.0 Off |                    0 |
| N/A   41C    P0             40W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-SXM2-32GB           On  |   00000000:8A:00.0 Off |                    0 |
| N/A   35C    P0             40W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-SXM2-32GB           On  |   00000000:B2:00.0 Off |                    0 |
| N/A   23C    P0             38W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
==================================
FUSED kernel training started
Samples per epoch: 5000
Batch size: 64
Learning rate: 0.005
Max gradient norm: 1.0
==================================



Using FUSED kernels (LayerNorm + Attn_Softmax)
{
    "data_size": {
        "train": 97976,
        "validation": 4512,
        "test": 100
    }
}

================================================================================
TRAINING START
================================================================================
Start time: 2025-10-17 04:30:38
================================================================================

Forward: 6.296079397201538
Backward: 17.619571685791016
Opt.step: 6.275184631347656
Forward: 5.550617456436157
Backward: 15.683067798614502
Opt.step: 6.05120325088501
Forward: 5.478873252868652
Backward: 16.872507095336914
Opt.step: 6.1478657722473145
Forward: 5.660819053649902
Backward: 15.526509761810303
Opt.step: 6.03979754447937
Forward: 5.601121664047241
Backward: 15.452893018722534
Opt.step: 5.939045429229736
Forward: 5.498202323913574
Backward: 15.581698656082153
Opt.step: 5.991816997528076
Forward: 5.4964599609375
Backward: 15.390025615692139
Opt.step: 5.924722194671631
Forward: 5.397711992263794
Backward: 15.63859748840332
Opt.step: 5.991076231002808
Forward: 5.458384275436401
Backward: 15.982423305511475
Opt.step: 5.943997383117676
Forward: 5.294074535369873
Backward: 15.58076548576355
Opt.step: 6.0026116371154785
Forward: 5.485181093215942
Backward: 15.801314115524292
Opt.step: 5.968993425369263
Forward: 5.31074070930481
Backward: 15.032430171966553
Opt.step: 5.985557556152344
Forward: 5.384846210479736
Backward: 15.954080820083618
Opt.step: 5.965142488479614
Forward: 5.436743259429932
Backward: 15.130741596221924
Opt.step: 5.9649436473846436
Forward: 5.355769872665405
Backward: 15.536610126495361
Opt.step: 5.959364414215088
Forward: 5.4803173542022705
Backward: 15.150641202926636
Opt.step: 5.96233344078064
Forward: 5.3987767696380615
Backward: 15.906869411468506
Opt.step: 5.851500749588013
Forward: 5.431046724319458
Backward: 15.234825849533081
Opt.step: 5.80931544303894
Forward: 5.278493165969849
Backward: 15.775065660476685
Opt.step: 5.839097499847412
Forward: 5.3693108558654785
Backward: 14.132698774337769
Opt.step: 5.797061204910278
Forward: 5.1974036693573
Backward: 15.104271411895752
Opt.step: 5.8106935024261475
Forward: 5.327693939208984
Backward: 14.28645658493042
Opt.step: 5.811661243438721
Forward: 5.173421382904053
Backward: 15.00971245765686
Opt.step: 5.85284948348999
Forward: 5.384767770767212
Backward: 14.83552885055542
Opt.step: 5.819456577301025
Forward: 5.224564552307129
Backward: 14.318671226501465
Opt.step: 5.802495002746582
Forward: 5.210344552993774
Backward: 14.816771745681763
Opt.step: 5.828196287155151
Forward: 5.182917356491089
Backward: 14.328383207321167
Opt.step: 5.820488691329956
Forward: 5.2161030769348145
Backward: 14.469597339630127
Opt.step: 5.8227808475494385
Forward: 5.262102365493774
Backward: 14.402034759521484
Opt.step: 5.788673639297485
Forward: 5.295694589614868
Backward: 14.802095413208008
Opt.step: 5.790406942367554
Forward: 5.227280139923096
Backward: 14.745201110839844
Opt.step: 5.780770301818848
Forward: 5.186786413192749
Backward: 14.661718845367432
Opt.step: 5.809975624084473
Forward: 5.236251354217529
Backward: 14.679758787155151
Opt.step: 5.782792091369629
Forward: 5.165118217468262
Backward: 14.684278964996338
Opt.step: 5.789496898651123
Forward: 5.222667694091797
Backward: 14.739386320114136
Opt.step: 5.792680501937866
Forward: 5.22616982460022
Backward: 14.851075172424316
Opt.step: 5.808725118637085
Forward: 5.190111875534058
Backward: 14.98801875114441
Opt.step: 5.798322916030884
Forward: 5.226145029067993
Backward: 14.615240097045898
Opt.step: 5.780810117721558
Forward: 5.1338210105896
Backward: 14.702539682388306
Opt.step: 5.781545162200928
Forward: 5.2514119148254395
Backward: 14.973071575164795
Opt.step: 5.78401255607605
Forward: 5.171201467514038
Backward: 14.979577779769897
Opt.step: 5.806790828704834
Forward: 5.305737495422363
Backward: 15.379543781280518
Opt.step: 5.787587881088257
Forward: 5.235052824020386
Backward: 15.23988151550293
Opt.step: 5.796869516372681
Forward: 5.308894872665405
Backward: 15.020633935928345
Opt.step: 5.799021244049072
Forward: 5.247364044189453
Backward: 14.7714684009552
Opt.step: 5.8005030155181885
Forward: 5.243516683578491
Backward: 15.500691652297974
Opt.step: 5.828648090362549
Forward: 5.304897785186768
Backward: 14.883049488067627
Opt.step: 5.7912726402282715
Forward: 5.193987846374512
Backward: 15.10863208770752
Opt.step: 5.799828767776489
Forward: 5.2966179847717285
Backward: 14.794769048690796
Opt.step: 5.777362823486328
Forward: 5.175165891647339
Backward: 14.813634157180786
Opt.step: 5.814871072769165
Forward: 5.248446941375732
Backward: 15.065019130706787
Opt.step: 5.781172752380371
Forward: 5.184301376342773
Backward: 14.88936972618103
Opt.step: 5.785274982452393
Forward: 5.266733884811401
Backward: 15.495980501174927
Opt.step: 5.776350498199463
Forward: 5.31330943107605
Backward: 14.854818344116211
Opt.step: 5.790118217468262
Forward: 5.257767915725708
Backward: 14.64941930770874
Opt.step: 5.8145482540130615
Forward: 5.232101678848267
Backward: 15.303351879119873
Opt.step: 5.778486490249634
Forward: 5.282879829406738
Backward: 15.27154278755188
Opt.step: 5.80712890625
Forward: 5.252376556396484
Backward: 15.510447025299072
Opt.step: 5.767168045043945
Forward: 5.298350095748901
Backward: 15.135034799575806
Opt.step: 5.798583745956421
Forward: 5.305842638015747
Backward: 15.26753306388855
Opt.step: 5.764095783233643
Forward: 5.160367727279663
Backward: 15.48664665222168
Opt.step: 5.803358554840088
Forward: 5.237406253814697
Backward: 15.306707620620728
Opt.step: 5.787995100021362
Forward: 5.22126579284668
Backward: 15.39479112625122
Opt.step: 5.807305812835693
Forward: 5.298992395401001
Backward: 15.397172927856445
Opt.step: 5.803616285324097
Forward: 5.246160507202148
Backward: 15.451297283172607
Opt.step: 5.792523622512817
Forward: 5.324923276901245
Backward: 14.774777889251709
Opt.step: 5.798780202865601
Forward: 5.249304294586182
Backward: 15.451614379882812
Opt.step: 5.8037004470825195
Forward: 5.284134387969971
Backward: 14.711951494216919
Opt.step: 5.805756568908691
Forward: 5.277408599853516
Backward: 15.19636082649231
Opt.step: 5.782053232192993
Forward: 5.214289665222168
Backward: 14.642868041992188
Opt.step: 5.911320447921753
Forward: 5.225214242935181
Backward: 15.473705768585205
Opt.step: 5.80214786529541
Forward: 5.274921417236328
Backward: 15.235129833221436
Opt.step: 5.815246820449829
Forward: 5.30993127822876
Backward: 15.686752796173096
Opt.step: 5.782392978668213
Forward: 5.268646001815796
Backward: 14.833647966384888
Opt.step: 5.80275559425354
Forward: 5.22125244140625
Backward: 15.319089651107788
Opt.step: 5.805866003036499
Forward: 5.242714881896973
Backward: 14.864474296569824
Opt.step: 5.8079915046691895
Forward: 5.2518298625946045
Backward: 15.560884475708008
Opt.step: 5.814705848693848
Forward: 5.31611180305481
Backward: 15.167801856994629
Opt.step: 5.8151514530181885
Forward: 1.0604848861694336
Backward: 2.9764158725738525
Opt.step: 5.903743505477905
Epoch 0: Validation Loss = 5.334743499755859
Epoch 0: {'bleu': 6.004085597646386}

================================================================================
TRAINING COMPLETE
================================================================================
End time: 2025-10-17 05:17:45
Total training time: 2826.86 seconds (47.11 minutes)
Time per epoch: 2826.86 seconds
================================================================================

Timing results saved to: ./workdir_vocab10000_lr0.005_embd256/timing_results.json
==================================
FUSED kernel training complete
==================================
