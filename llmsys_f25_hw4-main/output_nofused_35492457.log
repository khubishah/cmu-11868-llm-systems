Fri Oct 17 05:14:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:15:00.0 Off |                    0 |
| N/A   25C    P0             39W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  Tesla V100-SXM2-32GB           On  |   00000000:16:00.0 Off |                    0 |
| N/A   28C    P0             39W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  Tesla V100-SXM2-32GB           On  |   00000000:3A:00.0 Off |                    0 |
| N/A   28C    P0             40W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  Tesla V100-SXM2-32GB           On  |   00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0             40W /  300W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
==================================
Non-FUSED kernel training started
Samples per epoch: 5000
Batch size: 64
Learning rate: 0.005
Max gradient norm: 1.0
==================================



Using NON-FUSED kernels (standard operations)
{
    "data_size": {
        "train": 97976,
        "validation": 4512,
        "test": 100
    }
}

================================================================================
TRAINING START
================================================================================
Start time: 2025-10-17 05:14:41
================================================================================

Forward: 6.140112400054932
Backward: 17.59147071838379
Opt.step: 5.976846933364868
Forward: 5.836067199707031
Backward: 16.34013056755066
Opt.step: 5.948617696762085
Forward: 5.760690450668335
Backward: 16.72122287750244
Opt.step: 5.989267349243164
Forward: 5.856048107147217
Backward: 16.112595558166504
Opt.step: 5.932972192764282
Forward: 5.658702850341797
Backward: 16.86504101753235
Opt.step: 5.912996768951416
Forward: 5.680043458938599
Backward: 16.662187099456787
Opt.step: 5.984060287475586
Forward: 5.7593770027160645
Backward: 16.891772985458374
Opt.step: 5.914844751358032
Forward: 5.61767315864563
Backward: 16.55632495880127
Opt.step: 5.978358507156372
Forward: 5.63107442855835
Backward: 17.142608642578125
Opt.step: 5.947295665740967
Forward: 5.628377676010132
Backward: 16.63093638420105
Opt.step: 5.959470510482788
Forward: 5.721888065338135
Backward: 17.14417266845703
Opt.step: 5.919776916503906
Forward: 5.604429483413696
Backward: 16.80317711830139
Opt.step: 6.000996112823486
Forward: 5.678147554397583
Backward: 16.983371019363403
Opt.step: 5.9187798500061035
Forward: 5.591214656829834
Backward: 16.58356523513794
Opt.step: 5.9348955154418945
Forward: 5.876581907272339
Backward: 16.60484218597412
Opt.step: 5.908729553222656
Forward: 5.561485528945923
Backward: 17.20912504196167
Opt.step: 5.945284843444824
Forward: 5.587842226028442
Backward: 16.76635479927063
Opt.step: 5.798629522323608
Forward: 5.605266571044922
Backward: 16.533308029174805
Opt.step: 5.827517747879028
Forward: 5.719660043716431
Backward: 16.87844443321228
Opt.step: 5.808511018753052
Forward: 5.632168531417847
Backward: 16.333380699157715
Opt.step: 5.819278955459595
Forward: 5.630551099777222
Backward: 16.488681316375732
Opt.step: 5.845005989074707
Forward: 5.628383159637451
Backward: 16.235641717910767
Opt.step: 5.813217401504517
Forward: 5.7323691844940186
Backward: 17.08864450454712
Opt.step: 5.830336570739746
Forward: 5.618096590042114
Backward: 16.158517122268677
Opt.step: 5.801936388015747
Forward: 5.609434604644775
Backward: 16.57973003387451
Opt.step: 5.808760404586792
Forward: 5.564772129058838
Backward: 16.264002561569214
Opt.step: 5.7843005657196045
Forward: 5.556917428970337
Backward: 16.425527572631836
Opt.step: 5.789790868759155
Forward: 5.6724560260772705
Backward: 15.989955186843872
Opt.step: 5.817164182662964
Forward: 5.5453009605407715
Backward: 16.647530555725098
Opt.step: 5.804380416870117
Forward: 5.634871006011963
Backward: 16.010366678237915
Opt.step: 5.907786130905151
Forward: 5.872920274734497
Backward: 17.026914358139038
Opt.step: 5.843916177749634
Forward: 5.698247194290161
Backward: 16.161163330078125
Opt.step: 5.832376480102539
Forward: 5.686027765274048
Backward: 16.56962823867798
Opt.step: 5.8121497631073
Forward: 5.575783967971802
Backward: 16.360348224639893
Opt.step: 5.830604314804077
Forward: 5.643002510070801
Backward: 15.80164361000061
Opt.step: 5.826394081115723
Forward: 5.585768222808838
Backward: 16.266072511672974
Opt.step: 5.865654706954956
Forward: 5.605187654495239
Backward: 16.160613536834717
Opt.step: 5.799713373184204
Forward: 5.571673393249512
Backward: 16.32129430770874
Opt.step: 5.838801383972168
Forward: 5.805022478103638
Backward: 16.147011756896973
Opt.step: 5.811063051223755
Forward: 5.550751447677612
Backward: 16.288018465042114
Opt.step: 5.821667671203613
Forward: 5.667762517929077
Backward: 16.3257794380188
Opt.step: 5.810393333435059
Forward: 5.5676960945129395
Backward: 16.3483726978302
Opt.step: 5.823333024978638
Forward: 5.738754749298096
Backward: 16.405904054641724
Opt.step: 5.803308010101318
Forward: 5.58232855796814
Backward: 16.341826915740967
Opt.step: 5.857999324798584
Forward: 5.668377876281738
Backward: 16.858880043029785
Opt.step: 5.8261330127716064
Forward: 5.65028977394104
Backward: 16.07487964630127
Opt.step: 5.815037965774536
Forward: 5.788575887680054
Backward: 17.00921893119812
Opt.step: 5.822451114654541
Forward: 5.6572935581207275
Backward: 16.39874768257141
Opt.step: 5.8099610805511475
Forward: 5.653611660003662
Backward: 16.903648376464844
Opt.step: 5.834488868713379
Forward: 5.607030153274536
Backward: 16.67240023612976
Opt.step: 5.834012031555176
Forward: 5.712353706359863
Backward: 17.008381366729736
Opt.step: 5.840041160583496
Forward: 5.682624340057373
Backward: 16.669007301330566
Opt.step: 5.831223487854004
Forward: 5.68322229385376
Backward: 16.64104437828064
Opt.step: 5.819454669952393
Forward: 5.599004745483398
Backward: 16.489277124404907
Opt.step: 5.828401565551758
Forward: 5.792989015579224
Backward: 16.680574893951416
Opt.step: 5.847594738006592
Forward: 5.639350652694702
Backward: 16.81234335899353
Opt.step: 5.840359449386597
Forward: 5.733509302139282
Backward: 16.642892360687256
Opt.step: 5.815585136413574
Forward: 5.645984649658203
Backward: 16.932036638259888
Opt.step: 5.843295097351074
Forward: 5.623514652252197
Backward: 16.802810668945312
Opt.step: 5.7967588901519775
Forward: 5.669135570526123
Backward: 16.746782064437866
Opt.step: 5.859129190444946
Forward: 5.654466390609741
Backward: 16.832097053527832
Opt.step: 5.818386554718018
Forward: 5.639055967330933
Backward: 16.762486457824707
Opt.step: 6.133360385894775
Forward: 6.012377023696899
Backward: 17.014323472976685
Opt.step: 5.875545263290405
Forward: 5.968499660491943
Backward: 16.910279273986816
Opt.step: 5.846388101577759
Forward: 5.833353281021118
Backward: 16.73197102546692
Opt.step: 5.809493541717529
Forward: 5.709788799285889
Backward: 16.546827793121338
Opt.step: 5.802279949188232
Forward: 5.561196804046631
Backward: 16.82226538658142
Opt.step: 5.800523519515991
Forward: 5.669652223587036
Backward: 16.147913694381714
Opt.step: 5.803823709487915
Forward: 5.555699825286865
Backward: 16.401581525802612
Opt.step: 5.794909715652466
Forward: 5.632471561431885
Backward: 16.22246289253235
Opt.step: 5.812049150466919
Forward: 5.54551887512207
Backward: 16.5358669757843
Opt.step: 5.806257486343384
Forward: 5.679661512374878
Backward: 16.599363327026367
Opt.step: 5.810821771621704
Forward: 5.537525177001953
Backward: 16.403193473815918
Opt.step: 5.802117347717285
Forward: 5.667770862579346
Backward: 16.54333758354187
Opt.step: 5.829103469848633
Forward: 5.583386659622192
Backward: 16.64976406097412
Opt.step: 5.816376686096191
Forward: 5.67766261100769
Backward: 16.248615980148315
Opt.step: 5.824113368988037
Forward: 5.569817304611206
Backward: 16.294460773468018
Opt.step: 5.806830883026123
Forward: 5.64106297492981
Backward: 16.208213567733765
Opt.step: 5.814210653305054
Forward: 1.3019938468933105
Backward: 3.4336984157562256
Opt.step: 5.925902843475342
Epoch 0: Validation Loss = 5.327500820159912
Epoch 0: {'bleu': 5.5704866626846545}

================================================================================
TRAINING COMPLETE
================================================================================
End time: 2025-10-17 06:06:23
Total training time: 3102.40 seconds (51.71 minutes)
Time per epoch: 3102.40 seconds
================================================================================

Timing results saved to: ./workdir_vocab10000_lr0.005_embd256/timing_results.json
==================================
Non-FUSED kernel training complete
==================================
